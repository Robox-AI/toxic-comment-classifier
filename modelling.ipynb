{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from bin.imports import *\n",
    "from bin.config import *\n",
    "from bin.utils import *\n",
    "from bin.models import GRU_LSTM_model, CV_predictor, CAPSULE_model, DPCNN_model\n",
    "from bin.text_cleaner import TextCleaner\n",
    "from bin.contractions import contractions, negative_100, positive_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_100 = [i for i in negative_100 if len(i) > 3][:200]\n",
    "positive_100 = [i for i in positive_100 if len(i) > 3][:200]\n",
    "# negative_100 = sorted(negative_100, key= lambda x: -len(x))\n",
    "# positive_100 = sorted(positive_100, key= lambda x: -len(x))\n",
    "\n",
    "valuable_words = negative_100 + positive_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unify_tokens(comment):\n",
    "    nl = []\n",
    "    wl = WordNetLemmatizer().lemmatize\n",
    "    for word in comment:\n",
    "        word = wl(wl(word, pos='v'), pos='a')\n",
    "        # token in original form, exact matching\n",
    "        in_dict = emb_keys.get(word)\n",
    "        if in_dict is not None:\n",
    "            nl.append(word) \n",
    "        else:\n",
    "            word = word.lower()\n",
    "            word = wl(wl(word, pos='v'), pos='a')\n",
    "            # token lowercased, exact matching\n",
    "            in_dict = emb_keys.get(word)\n",
    "            if in_dict is not None:\n",
    "                nl.append(word)\n",
    "            else:\n",
    "                # break if work consist of < 3 symbols as non reliable solution\n",
    "                if len(word) < 3:\n",
    "                    continue\n",
    "                # top pos/neg words by LR weights lowercased, partial matching\n",
    "                for w in valuable_words:\n",
    "                    if w in word:\n",
    "                        word = word.replace(w, '')\n",
    "                        nl.append(w)\n",
    "                        if len(word) < 3:\n",
    "                            continue\n",
    "                # embedding keys lowercased, partial matching\n",
    "                for w in emb_sorted:\n",
    "                    if w.lower() in word:\n",
    "                        word = word.replace(w, '')\n",
    "                        nl.append(w.lower())\n",
    "                        if len(word) < 3:\n",
    "                            continue\n",
    "        # words which were not found in dict gonna be excluded from the comment\n",
    "    return nl\n",
    "\n",
    "def process_comment(df):\n",
    "    df['comment_text'] = df['comment_text'].apply(unify_tokens).values\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/ser/Downloads/fasttext/crawl-300d-2M.vec'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3d695ddd3056>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mEMBEDDING_FILE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/ser/Downloads/fasttext/crawl-300d-2M.vec'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0memb_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_emb_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEMBEDDING_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0memb_sorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m15\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/ser/Downloads/fasttext/crawl-300d-2M.vec'"
     ]
    }
   ],
   "source": [
    "EMBEDDING_FILE = '/home/ser/Downloads/fasttext/crawl-300d-2M.vec'\n",
    "emb_keys = dict(get_emb_dict(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n",
    "emb_sorted = [i for i in sorted(emb_keys, key=lambda x: -len(x)) if len(i) < 15 and len(i)>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv.zip').fillna(\"fillna\")\n",
    "test = pd.read_csv('../data/test.csv.zip').fillna(\"fillna\")\n",
    "submission = pd.read_csv('../data/sample_submission.csv.zip')\n",
    "\n",
    "print (f'text cleaner processing: {datetime.datetime.now()}')\n",
    "tc = TextCleaner(contractions)\n",
    "train['comment_text'] = tc.transform(train['comment_text'].fillna('na').values)\n",
    "test['comment_text'] = tc.transform(test['comment_text'].fillna('na').values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train cleaning: 2018-03-19 19:10:07.279024\n"
     ]
    }
   ],
   "source": [
    "print (f'train cleaning: {datetime.datetime.now()}')\n",
    "train = parallelize_dataframe(train, process_comment)\n",
    "print (f'test cleaning: {datetime.datetime.now()}')\n",
    "test = parallelize_dataframe(test, process_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = '/home/ser/Downloads/fasttext/crawl-300d-2M.vec'\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 150000\n",
    "maxlen = 150\n",
    "embed_size = 300\n",
    "\n",
    "X_train = train[\"comment_text\"].values\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "X_test = test[\"comment_text\"].values\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features, lower=False)\n",
    "tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "missed = []\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: \n",
    "        embedding_matrix[i] = embedding_vector      \n",
    "    else:\n",
    "        missed.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(missed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRU_LSTM_model(CuDNNLSTM, maxlen, max_features, embed_size, embedding_matrix)\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.9, random_state=233)\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                 callbacks=[RocAuc], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/15\n",
      "143613/143613 [==============================] - 20s 140us/step - loss: 0.0733 - acc: 0.9742 - val_loss: 0.0500 - val_acc: 0.9814\n",
      "\n",
      " ROC-AUC - epoch: 1 - score: 0.971133 \n",
      "\n",
      "Epoch 2/15\n",
      "143613/143613 [==============================] - 17s 120us/step - loss: 0.0495 - acc: 0.9815 - val_loss: 0.0463 - val_acc: 0.9825\n",
      "\n",
      " ROC-AUC - epoch: 2 - score: 0.979113 \n",
      "\n",
      "Epoch 3/15\n",
      "143613/143613 [==============================] - 18s 122us/step - loss: 0.0460 - acc: 0.9824 - val_loss: 0.0473 - val_acc: 0.9824\n",
      "\n",
      " ROC-AUC - epoch: 3 - score: 0.981239 \n",
      "\n",
      "Epoch 4/15\n",
      "143613/143613 [==============================] - 18s 123us/step - loss: 0.0438 - acc: 0.9831 - val_loss: 0.0431 - val_acc: 0.9835\n",
      "\n",
      " ROC-AUC - epoch: 4 - score: 0.983787 \n",
      "\n",
      "Epoch 5/15\n",
      "143613/143613 [==============================] - 17s 122us/step - loss: 0.0413 - acc: 0.9839 - val_loss: 0.0430 - val_acc: 0.9834\n",
      "\n",
      " ROC-AUC - epoch: 5 - score: 0.984971 \n",
      "\n",
      "Epoch 6/15\n",
      "143613/143613 [==============================] - 17s 118us/step - loss: 0.0397 - acc: 0.9843 - val_loss: 0.0438 - val_acc: 0.9837\n",
      "\n",
      " ROC-AUC - epoch: 6 - score: 0.984433 \n",
      "\n",
      "Epoch 7/15\n",
      "143613/143613 [==============================] - 18s 122us/step - loss: 0.0379 - acc: 0.9848 - val_loss: 0.0425 - val_acc: 0.9830\n",
      "\n",
      " ROC-AUC - epoch: 7 - score: 0.986733 \n",
      "\n",
      "Epoch 8/15\n",
      "143613/143613 [==============================] - 17s 120us/step - loss: 0.0365 - acc: 0.9852 - val_loss: 0.0426 - val_acc: 0.9838\n",
      "\n",
      " ROC-AUC - epoch: 8 - score: 0.985978 \n",
      "\n",
      "Epoch 9/15\n",
      "143613/143613 [==============================] - 17s 120us/step - loss: 0.0350 - acc: 0.9857 - val_loss: 0.0429 - val_acc: 0.9837\n",
      "\n",
      " ROC-AUC - epoch: 9 - score: 0.987321 \n",
      "\n",
      "Epoch 10/15\n",
      "143613/143613 [==============================] - 17s 121us/step - loss: 0.0337 - acc: 0.9862 - val_loss: 0.0473 - val_acc: 0.9807\n",
      "\n",
      " ROC-AUC - epoch: 10 - score: 0.987668 \n",
      "\n",
      "Epoch 11/15\n",
      "143613/143613 [==============================] - 17s 121us/step - loss: 0.0324 - acc: 0.9866 - val_loss: 0.0475 - val_acc: 0.9832\n",
      "\n",
      " ROC-AUC - epoch: 11 - score: 0.986771 \n",
      "\n",
      "Epoch 12/15\n",
      "143613/143613 [==============================] - 17s 121us/step - loss: 0.0315 - acc: 0.9869 - val_loss: 0.0439 - val_acc: 0.9832\n",
      "\n",
      " ROC-AUC - epoch: 12 - score: 0.987882 \n",
      "\n",
      "Epoch 13/15\n",
      "143613/143613 [==============================] - 17s 120us/step - loss: 0.0303 - acc: 0.9874 - val_loss: 0.0454 - val_acc: 0.9823\n",
      "\n",
      " ROC-AUC - epoch: 13 - score: 0.987454 \n",
      "\n",
      "Epoch 14/15\n",
      "143613/143613 [==============================] - 18s 122us/step - loss: 0.0299 - acc: 0.9876 - val_loss: 0.0456 - val_acc: 0.9834\n",
      "\n",
      " ROC-AUC - epoch: 14 - score: 0.987154 \n",
      "\n",
      "Epoch 15/15\n",
      "143613/143613 [==============================] - 17s 120us/step - loss: 0.0287 - acc: 0.9881 - val_loss: 0.0516 - val_acc: 0.9828\n",
      "\n",
      " ROC-AUC - epoch: 15 - score: 0.985625 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f959a4426a0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpcnn_kwargs = {\n",
    "    'maxlen': maxlen,\n",
    "    'max_features': max_features,\n",
    "    'embed_size': embed_size,\n",
    "    'embedding_matrix': embedding_matrix,\n",
    "    'spatial_dropout': 0.25,\n",
    "    'filter_nr': 64,\n",
    "    'filter_size': 3, \n",
    "    'max_pool_size': 3, \n",
    "    'max_pool_strides': 2,\n",
    "    'dense_nr': 256,\n",
    "    'dense_dropout': 0.5\n",
    "}\n",
    "\n",
    "model = DPCNN_model(**dpcnn_kwargs)\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.9, random_state=233)\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                 callbacks=[RocAuc], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DPCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dpcnn_kwargs = {\n",
    "    'maxlen': maxlen,\n",
    "    'max_features': max_features,\n",
    "    'embed_size': embed_size,\n",
    "    'embedding_matrix': embedding_matrix,\n",
    "    'spatial_dropout': 0.25,\n",
    "    'filter_nr': 64,\n",
    "    'filter_size': 3, \n",
    "    'max_pool_size': 3, \n",
    "    'max_pool_strides': 2,\n",
    "    'dense_nr': 256,\n",
    "    'dense_dropout': 0.5\n",
    "}\n",
    "\n",
    "batch_size = 128\n",
    "n_splits = 10\n",
    "epochs = range(10)\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "cv = CV_predictor(DPCNN_model, x_train, y_train, x_test, \n",
    "                      n_splits, batch_size, epochs, list_classes, dpcnn_kwargs)\n",
    "cv.predict()\n",
    "\n",
    "train_p = cv.train_predictions\n",
    "test_p = cv.test_predictions\n",
    "test_p.index = test['id']\n",
    "\n",
    "train_p.to_csv('/home/ser/DL/toxic/train_predictions/f_dpcnn.csv', index=False)\n",
    "test_p.reset_index().to_csv('/home/ser/DL/toxic/test_predictions/f_dpcnn.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gru_kwargs = {\n",
    "    'CuDNN': CuDNNGRU,\n",
    "    'maxlen': maxlen, \n",
    "    'max_features': max_features, \n",
    "    'embed_size': embed_size, \n",
    "    'embedding_matrix' : embedding_matrix\n",
    "}\n",
    "\n",
    "batch_size = 128\n",
    "n_splits = 10\n",
    "epochs = range(4)\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "cv = CV_predictor(GRU_LSTM_model, x_train, y_train, x_test, \n",
    "                      n_splits, batch_size, epochs, list_classes, gru_kwargs)\n",
    "cv.predict()\n",
    "\n",
    "train_p = cv.train_predictions\n",
    "test_p = cv.test_predictions\n",
    "test_p.index = test['id']\n",
    "\n",
    "train_p.to_csv('/home/ser/DL/toxic/train_predictions/f_gru_lem_low.csv', index=False)\n",
    "test_p.reset_index().to_csv('/home/ser/DL/toxic/test_predictions/f_gru_lem_low.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gru_kwargs = {\n",
    "    'CuDNN': CuDNNLSTM, \n",
    "    'maxlen': maxlen, \n",
    "    'max_features': max_features, \n",
    "    'embed_size': embed_size, \n",
    "    'embedding_matrix' : embedding_matrix\n",
    "}\n",
    "\n",
    "batch_size = 128\n",
    "n_splits = 10\n",
    "epochs = range(4)\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "cv = CV_predictor(GRU_LSTM_model, x_train, y_train, x_test, \n",
    "                      n_splits, batch_size, epochs, list_classes, gru_kwargs)\n",
    "cv.predict()\n",
    "\n",
    "train_p = cv.train_predictions\n",
    "test_p = cv.test_predictions\n",
    "test_p.index = test['id']\n",
    "\n",
    "train_p.to_csv('/home/ser/DL/toxic/train_predictions/f_lstm_lem_low.csv', index=False)\n",
    "test_p.reset_index().to_csv('/home/ser/DL/toxic/test_predictions/f_lstm_lem_low.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Capsule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "capsule_kwargs = {\n",
    "    'maxlen': maxlen, \n",
    "    'max_features': max_features, \n",
    "    'embed_size': embed_size, \n",
    "    'embedding_matrix' : embedding_matrix, \n",
    "    'rate_drop_dense': 0.3,\n",
    "    'Num_capsule': 10, \n",
    "    'Dim_capsule': 16, \n",
    "    'Routings':  5,\n",
    "    'gru_len': 128\n",
    "}\n",
    "\n",
    "batch_size = 128\n",
    "n_splits = 10\n",
    "epochs = range(4)\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "cv = CV_predictor(CAPSULE_model, x_train, y_train, x_test, \n",
    "                      n_splits, batch_size, epochs, list_classes, capsule_kwargs)\n",
    "cv.predict()\n",
    "\n",
    "train_p = cv.train_predictions\n",
    "test_p = cv.test_predictions\n",
    "test_p.index = test['id']\n",
    "\n",
    "train_p.to_csv('/home/ser/DL/toxic/train_predictions/f_capsule_lem_low.csv', index=False)\n",
    "test_p.reset_index().to_csv('/home/ser/DL/toxic/test_predictions/f_capsule_lem_low.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
